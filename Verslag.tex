\documentclass[11pt]{article}

\usepackage[left=3cm, right=3cm, top=3cm, bottom=3cm]{geometry}
\usepackage{pdfpages}
\usepackage{cite}
\usepackage{eurosym}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{array}
\usepackage{graphicx}
\usepackage{apacite}
\usepackage{gensymb}

\graphicspath{ {figures/} }
\graphicspath{ {./verslag_figuren/} }

\usepackage[numbib]{tocbibind}

%%%%% voor implementeren matlabcode in latex %%%%%

\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\includepdf[pages=-]{frontpage.pdf}

\section*{Abstract}
\thispagestyle{empty}

\newpage
\tableofcontents
\thispagestyle{empty}

\newpage
\listoftables
\thispagestyle{empty}

\newpage
\listoffigures
\thispagestyle{empty}

\newpage
\section{Introduction}
\pagenumbering{arabic}
Digital image processing has been a crucial part of the current digitalisation movement. From industrial machinery to customer amusement, the vision of computer-aided systems has become a given for most users. While image alteration and manipulation remain a core part of this image processing, nowadays other image related problems are being solved by artificial intelligence. Most were considered to be an important part of digital image processing. Among these, the problem of this paper can be found: feature extraction. The ability to count objects in an image to be more exact. So why use 'traditional' methods to solve this problem? While being a great way for unravelling many problems, artificial intelligence mostly provides general solutions. However, certain cases are solved more efficiently by specific schemes. Such is the case with object counting: while deep learning algorithms need a big data set as training material, standard image processing only requires the image itself.\\
Regardless which way a method processes images, it needs a visual source. In this paper the focus is on live object counting, which is only possible with a camera. Evidently, the choice of hardware greatly impacts the methods that can be used. This choice will be covered in !TITEL HARDWARE HERE!.\\
By far the most important part of this task is the algorithm by which the items in the picture will be numbered. Classically, object counting algorithms have a standard group of steps: filtering, converting to an intensity matrix, edge detection, converting to a binary matrix, boundary boxing and the counting itself. These segments don't have a fixed order and can occur multiple times in the final method. Most of these steps can also be approached in different ways. A wide range of possible filters, kernels, edge detection methods, etc. exist, which all have their benefits and drawbacks.!REFERENTIE BOEK! These choices will be discussed in !TITEL SOFTWARE HERE!.\\
These methods, while being the core of the solution, are fairly simple to implement with the use of libraries or built-in functions. In this paper is opted to give a full implementation of these functions, limiting the usage of libraries to the minimum, in !TITEL IMPLEMENTATION HERE!. If the functions are deemed to be basic, only a simple explanation will be given.

\section{Problem Description}

The object counting system described in this report is capable of counting non-moving objects in a basket. These objects can vary in shape, size and colour. The colour of both the basket and its contents are free from restrictions as well as the shape of the objects.\\
In the primary stage of this paper, not all these variables are taken into account. The simplest objects, which the system is required to count, are rectangles, cylinders and circles, all with a uniform colour. If possible, the circumference of these objects can be outlined and measured as shown in Fig.~\ref{fig:example}.\\ All of this is done in real-time and with a budget of \euro 250.

\begin{figure}[h]
\centering
  \includegraphics[width=0.7\linewidth]{opdracht.png}
  \caption{The example included in the assignment.}
  \label{fig:example}
\end{figure}



\section{Design}
\subsection{Hardware}
The hardware to create a system as described above, is not complicated. In essence, it consists of a computer, a camera and a cable, to transfer the data between the prior named necessities. Each of these hardware components is discussed in the following section.\\
Choosing the camera is a vital element in this project. If chosen poorly, it can fiercely limit the outcome of the final algorithm. There are three main options for visual input: an ordinary webcam, an industrial camera or a camera with built-in depth sensors. Each with its pros and cons. A webcam is cheap and readily available but does not assure good image quality and easy access to its data. A camera for industrial usage is rather expensive, especially with a budget of \euro 250. Industrial grade options which are cheap enough exist, but these models deliver their images in greyscale. This greatly limits the possible methods which can be used. Thirdly, the depth sensing cameras are available in a reasonable price range and deliver ,overall, good quality data. Moreover these models have the added benefit of depth sensor which, in contrast to the previous option, adds more possible ways to solve the problem.\\
Having considered all of the above, the best option is the latter one. More specifically, the system described in this report is based on a Kinect V2 from Microsoft. This camera has a color lens with a resolution of $1920$ by $1080$ pixels and a corresponding field of view of $84.1\degree$ by $53.8\degree$(REFERENTIE Smeenk). The high resolution ensures an accurate matrix representation of the real image. Each color frame pulled from the Kinect V2 is represented by an array structure of $1080$x$1920$x$3$. Every element corresponds with a pixel of the image and varies between $0$ and $255$. Obviously it can be separated into three different matrices each belonging to $\mathbb{R}^{2}$ and based on a different colour: red, green or blue.\\ Next to the colour camera, the Kinect also possesses a depth sensor. An infrared projector and camera make this possible(REFERENCE researchgate). It provides a $424$x$512$ array making the depth image one of roughly $200 000$ pixels. The field of view of this function is $70.6\degree$ by $60\degree$. Note that the depth camera provides data about parts of the environment that the color camera does not see, and vice versa. When the computer reads the depth data, every number in the matrix represents a distance in millimetres. Obviously there are some restrictions. This technology only provides correct information if the object is at a distance located in between half a meter and 4 meters. This has to be taken into account for further implementation of this paper.\\
As second element of hardware the computer has a less important role. Preferably, OSX isn't used as operating system for this application because the Kinect drivers do not exist for Macintosh computers. If the reader has a Mac, problems can be avoided by running either Windows or Ubuntu via a virtual machine. The algorithm should run in an acceptable time frame on every machine.\\
To conclude this section a brief word on the necessary transfer cable. Since a depth sensing camera is used, two types of data (depth and color) need to be transferred. The Microsoft OEM Kinect Adapter makes this possible. The special adapter is the only available option and consists of two general parts. One part for delivering current to the camera and the other to transfer both types of data to the connected computer.


\subsection{Software}
There are a lot of options when it comes to software and a wide range of different algorithms for image processing exist. The diagram on FIG…XX… shows a couple of different methods. There is no ‘right way’ to count objects in an image. Different approaches have their own advantages and disadvantages. The only general ideas that are common throughout most algorithms are:
\begin{itemize}
\item Converting the RGB image to greyscale
\item Run filters over the image to remove noise
\end{itemize}
These elements are also visible in the diagram(VERWIJZING NR DIAGRAM). In the next scope, three general methods are featured and briefly discussed. Each was investigated in prospect of this paper. 
\paragraph{Method 1}\mbox{}\\
This method is the most simple and straightforward to implement. As input it requires a filtered greyscale image. This is passed trough a thresholding algorithm with a pre-defined threshold value. The output is a binary matrix. This array only has 0's and 1's as elements, respectively representing the colours black and white. The key to solving the problem in this specific scheme is writing code that finds the threshold value based on environmental parameters. When in possession of a truly black and white image, a simple edge detection program is run which makes the edges visible.
\\Advantages: It’s an easy and fast algorithm.
\\Disadvantages: With a pre-defined threshold value it just classifies pixels based on colour. A dynamic value is required.
\paragraph{Method 2}\mbox{}\\
The second method tackles the colour analysis in the opposite order than the first method, as it starts with an edge detection algorithm. Since the input image is still very complex, this edge detection is way more comprehensive. The output is a greyscale image, contrary to the binary array the reader might expect. This is followed by some thresholding code with a pre-defined threshold value. The current image is now represented by a matrix where the edges are outlined using binary elements. Based on the fact that there is a lot of noise using this sequence of steps, it's recommended to include noise reduction code.
\\Advantages: It detects all kinds of objects, not based on colour or shape.
\\Disadvantages: The boundary between different objects needs to be clear for this to work.
\paragraph{Method 3}\mbox{}\\
The third way takes a different approach to solving the analysis of the colour image. When using this, a compromise in functionality is made. Since it needs a picture of the empty background without any objects, the user experience is worsened. After getting a background image, the picture of the situation with objects gets filtered and the algorithm converts it into a greyscale image. Using this less complex matrix, the code loops through the image pixel by pixel. This necessary but time consuming loop checks if the pixel on the image is more or less the same as the corresponding pixel on the background image. If located within a pre-determined range, that element of the array gets classified as background. The consequence is that the output is a binary image with clear-cut objects.
\\Advantages: It is very good in detecting objects, not being based on colour or shape.
\\Disadvantages: There needs to be an image of the empty background. Note that the lighting conditions have to be unaffected in between taking the needed pictures for this algorithm.


\paragraph{Implementation}\mbox{}\\
After comparing these methods, the second method comes out as the better of the three. See Fig.\ref{fig:comparison_methods} for the comparison.\\
The first step, as seen above, is to convert the image to greyscale \cite{greyscale}. This is easily done by a calculating a weighted average of the values of all three red, green and blue matrices as shown in the following equation.
\begin{figure}[h]
	\center
  \includegraphics[width=0.7\linewidth]{comparison_methods.png}
  \caption{A comparison between the 3 different methods.}
  \label{fig:comparison_methods}
\end{figure}
\begin{equation}
greyscale\_image(row, col) = 0.2989 * RED + 0.5870 * GREEN + 0.1140 * BLUE
\end{equation}
The weights used count up to $1$ so the values in the greyscale image can vary from $0$ to $255$. All these values $greyscale\_image(row, col)$ form the new image.

Before running the image through an edge detection algorithm, two filters are applied. Both blur the image to an extent such that noise after edge detection is considerably reduced. This effect is visualised in Fig. \ref{fig:filter_comparison}
Firstly a Gaussian blur is applied. Most filters are a convolution of a kernel with the image. For a Gaussian blur the G kernel below is used. This is just a weighted average. The pixels centered around the main pixel have bigger weights than at the edges.
\begin{figure}[h]
	\center
  \includegraphics[width=0.7\linewidth]{filter_comparison.png}
  \caption{A comparison with the use of filters.}
  \label{fig:filter_comparison}
\end{figure}

\begin{equation}
  G = (1/159) * 
  \begin{bmatrix}
   2 & 4 & 5 & 4 & 2\\
   4 & 9 & 12 & 9 & 4\\
   5 & 12 & 15 & 12 & 5\\
   4 & 9 & 12 & 9& 4\\
   2 & 4 & 5 & 4 & 2
  \end{bmatrix}
\end{equation}
The blur can be applied by doing a convolution of the G matrix (the kernel) on the image matrix.
The second blur is a mean blur \cite{mean}. This is just the same, just another kernel. This kernel calculates the average of the values around the pixel.
\begin{equation}
M = (1/9) * 
\begin{bmatrix}
	1&1&1\\
	1&1&1\\
	1&1&1
\end{bmatrix}
\end{equation}
Note that both G and M have a norm of 1. If this wasn't the case pixel values of the filtered image could exceed the boundary values of 0 to 255. //
After both filters the image is ready to run through an edge detection algorithm\cite{edge_detection}. This algorithm is itself also a filter with kernel given by the matrix L below.
It calculates the \textit{spatial  derivative} or in simpler words, it highlights regions of rapid intensity change.
\begin{equation}
L =\begin{bmatrix}
	0&-1&0\\
	-1&4&-1\\
	0&-1&0
\end{bmatrix}
\end{equation}


Note now how the kernel uses the pixels next to the evaluated pixel to see how much intensity changes. If the image wouldn't have been filter before convolution with L, more 'edges' would have been drawn.\\
Note also how this convolution returns a new image which can have negative values for its pixels. The more negative the value, the darker the image.\\
The threshold algorithm, used in the following step, is based on this feature. This algorithm runs through to whole matrix and assigns each value with either a 0 or a 1. It decides this by assessing if the current value is either smaller than or bigger than a threshold value, respectively. After conducting multiple experiments and testing, a threshold value of 2 seems to do the trick. After applying the algorithm, the matrix becomes a binary image with only the edges in white. Based on these edges it is possible to outline the objects and count them, but further research and programming has to be done to complete the whole program.

\subsubsection{Analysis Depth Sensor}
Using only the RGB image does have some shortcomings. Iit is rather difficult to distinguish an object from its shadow, a multicoloured object could be seen as multiple different objects and a lot of reflection could make an object undetectable. These are some of the reasons why enrichening the object counting algorithm with the usage of a depth sensor is advised. Like featured in the section about the hardware, each element of the input data represents a distance in millimeters.\\
Firstly the code should be able to provide a clear difference in height between the objects and the background using the depth data. This is followed with a filter to get rid of the existing noise reduction. At last, the filtered matrix will be used to detect the edges of the objects and thus detect the items themselves. \textbf{The code that accompanies this description, can be found at page...}
 
\paragraph{Detection of the difference in height}\mbox{}\\
The goal is to see a clear difference between the objects and the background. This can be achieved in different ways: it is possible to use a threshold and label everything closer than this predetermined distance as an object. A disadvantage of this method is that this value will be different for different vertical positions of the kinect v2. Also, the image of the sensor contains some noise. For example: a picture of a big flat table will not be viewed as a equidistant surface. The elements of the matrix will be different. Another, and more preferred, method would be to use a Sobel-Feldman operator \textbf{[hier komt verwijzing naar boek in bronvermelding]}. This operation approximates the gradient in each of the points of the matrix, and gives an idea where there is a sudden difference in height (thus where there might be an object). It works by convolving 2 kernels with the image matrix A to become $G_{x}$ and $G_{y}$: respectively one for the horizontal and one for the vertical change in height: 

\begin{eqnarray*}
G_{x} &=& 
	\begin{bmatrix}
		1&0&-1\\
		2&0&-2\\
		1&0&-1
	\end{bmatrix}
	*A \\
G_{y} &=& 
	\begin{bmatrix}
		1&2&1\\
		0&0&0\\
		-1&-2&-1
	\end{bmatrix}
	*A\\
G &=& \sqrt{G_{x}^2+G_{y}^2}
\end{eqnarray*}
In the last equation, G is the magnitude of the total gradient as well as the value inserted in the new matrix.

\begin{figure}[h]
	\center
  \includegraphics[width=1\linewidth]{original_and_sobel.png}
  \caption{the original RGB image (left) and the image after the Sobel-Feldman operator (right)}
  \label{fig:Moore_Neighbor}
\end{figure}

\paragraph{Filtering of the noise}\mbox{}\\
After adding al the different magnitudes of the gradients to an array, some anomalies still exist. There can be some impossible elements, like points that seem to be further away than the basket, or fluctuations in areas that are supposed to be flat (noise). The simplest way to solve this problem would be to use a maximum and minimum treshold: The maximum treshold can be a value that is further away than the basket. These values are impossible and the corresponding values in the matrix can be set to zero. The minimum treshold can be decided by empirical research. Values lower than this value can be seen as noise and thus can be set to zero.

\begin{figure}[h]
	\center
  \includegraphics[width=0.6\linewidth]{sobel_and_threshold.png}
  \caption{the original image after using a Sobel-Feldman operator and a threshold filter}
  \label{fig:Moore_Neighbor}
\end{figure}

\section{Implementation}
Throughout the project a lot of different approaches were tested and discarded. But in essence they all do the same thing they convert the original image to a binary image. In this binary image the objects are represented by one value and the background by another. Afterwards this binary image is analysed and a simple algorithm suffices to count the objects. In this fase of the program the same code is applicable. This code consists of a few important parts: the actual counting and the drawing of the boundary boxes. 

Throughout the project a lot of different approaches were tested and discarded. But in essence ,they all do the same thing. They convert the original image to a binary image. Afterwards, this binary array is analysed and a simple algorithm suffices to count the objects. In this fase of the program the same code is applicable. This code consists of a few important parts: the actual counting and the drawing of the boundary boxes. 
\paragraph{Counting of the objects}\mbox{}\\
The central objective of this paper is counting the amount of objects in a specific rectangular field of view. The general approach to this problem is converting the image to a binary image where black pixels represent the background and white pixels represent the objects. By counting the groups of pixels it is possible to know how many objects the original image contains. In the image processing toolbox for matlab there exist a few functions that come in really handy for this kind of tasks. One of these functions bwlabel actually counts group of pixels of at least 8 that are connected. The syntax of this function goes as follows: 
\begin{equation}
[L, num] = bwlabel(BW)
\end{equation}
where BW represents the binary (or black and white) image; num represents the number of objects in the BW image and where L represents a matrix were the first group of pixels are numbered 1, the second group 2 etc. that way it's easier to get a count for how many objects there are.

\paragraph{Boundary boxes}\mbox{}\\
The image processing toolbox really simplifies the drawing of boundary boxes. Once a binary image is obtained the function regionprops can extract properties about image regions. Where image regions are defined as 8-connected components in an binary image. This means that each image region contains at least 8 interconnected white pixels, since the black pixels are registered as background. The property that's interesting for this part of the project is called 'boundingbox'. This property returns for every image region the smallest rectangle containing this region. In two dimensions this is a vector with 4 values, the x-coordinate of the upper left corner, the y-coordinate of that corner, the width and the height. The function 
\begin{equation}
rectangle('Position', pos)
\end{equation}
where 'Position' declares the input and where pos is the input obtained from regionprops, can easily display this boundingbox. 

\paragraph{Edge detection}\mbox{}\\
There are a lot of ways to implement edge detection. Edge detection algorithms as described in PARAGRAPH exist for greyscale image. But if a binary image is available, this becomes much easier. For starters there exists a function in the image processing toolbox called bwboundaries. The syntax of that function goes as follows: 
\begin{equation}
B = bwboundaries (BW)
\end{equation}
where BW represents the input, this is a binary image which only consists out of black and white pixels; and B represents the output, which consist out of a cell array with N elements (number of image regions in the binary image), all these elements contain a list of the boundary pixels. Which in turn are fairly easy to draw. They can be inserted in the matrix of the image by replacing values, this is done by looping through the cell arrays. The advantage of this method is that the image can actually be printed. When they are drawn on top of the image with a function like visboundaries the actual values of the pixels stay unchanged, but it become different figures. One with the image and another on top of it with the edges. 
The function bwboundaries implements the Moore-Neighbor tracing algorithm. The algorithm loops through the entire matrix until it finds a white pixel (a pixel that belongs to an image region). This pixel is defined as the start pixel. Once it finds a start pixel it searches for the next connected white pixel. This means another white pixel in one of the eight regions around the start. The algorithm does this by examining the pixels in a clockwise direction. Once it finds a new white pixel, this pixel is added to the sequence B and becomes our new start pixel. This process keeps on running until the algorithm visits the first start pixel for a second time. The only problem with this algorithm is that sometimes the first start pixel is visited for a second time before all of the outline is visited (See fig. 3).

\begin{figure}[h]
	\center
  \includegraphics[width=0.3\linewidth]{Moore_Neighbor.png}
  \caption{Problem with stopping criteria Moore-Neighbor tracing algorithm.}
  \label{fig:Moore_Neighbor}
\end{figure}

This problem is resolved with the Jacob's stopping criterion. Which states that the algorithm can stop once the first start pixel is visited out of the same direction as it was initially entered. This leaves four possibilities that need to be checked, from below, from the left, from above or from the right. With this additional criteria, every pixel at the edge of a connected region is visited. 
To find the edges of all the interconnected image regions this process is repeated until every pixel of the image matrix has been checked. 

\section{Further planning}

Being halfway trough the project, a visual timeframe is created. In the appendices, a Gantt chart can be found.(APPENDIX) 
This visualizes the current state of the solution for the described problem, as well as the schedule for the next few weeks. 
This project contains five milestones, two of them are already achieved. These have a minor value in prospect to the total paper though. The most important occupancy of the next weeks is implementing key elements of the final algorithm. Key components like filling the edges, creating the boundary boxes and eventually counting the number of objects still need the necessary attention. All of this while the given deadlines need to be respected. As it is possible to view in the chart, the decision to start rather early on the folder is made. A professional representation of the findings takes time. So planning it like this, ensures enough time to perfect the folder. \\
In general, the project is on schedule. From a critical point of view, too much time writing the report during the team sessions was wasted. For the final paper, more individual work is recommended and will happen.

\section{Budget management}

As seen above, the system explained in this paper primarily consists of software which on its own doesn't cost anything. On the contrary, the necessary hardware is rather costly. The current set-up consists of a tripod and the electronics. The tripod is lend for free by the faculty thus the only remaining costs are the Kinect v2 and its adapter to connect with a personal computer.\\
With a budget of 250 EURO, this is feasible. Both the Kinect and the adapter have been ordered but as of writing this paper, a fixed price isn't known. At the current market prices, the estimated cost is \euro 200. The remaining \euro 50 are a safe backup for other small costs.

\section{Course Integration}
For this project, some courses from the first three semesters are useful to be able to finish it. Linear algebra is used for working in the matrix the image represents. The programming in matlab is a lot easier after the course of computer science. Numerical Mathematics can be used for working with really large matrices. To make sure the system doesn't use to much memory, the course of information transmission and processing is used.

\section{Conclusion}

\section{List of references}
\section{References}

\nocite{*}
\bibliographystyle{apacite}
\bibliography{references.bib}

\section{Appendix}

%%%%% code RGB sensor %%%%%

\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{blue}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
}


\section*{Matlab Code RGB sensor}

\lstinputlisting{thresholding_and_edge.m}

%%%%% code depth sensor %%%%%

\section*{Matlab Code depth sensor}

\lstinputlisting{testDepth.m}



\end{document}
